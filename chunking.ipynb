{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd6ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "file_path='Data/mindset.pdf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79db1fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader(file_path)\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75dd0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57b641fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONTENTS\\nIntroduction\\n1.    THE MINDSETS\\nWhy Do People Differ?\\nWhat Does All This Mean for You? The Two Mindsets\\nA View from the Two Mindsets\\nSo, What’s New?\\nSelf-Insight: Who Has Accurate Views of Their Assets and\\nLimitations?\\nWhat’s in Store\\n2.    INSIDE THE MINDSETS\\nIs Success About Learning—Or Proving You’re Smart?\\nMindsets Change the Meaning of Failure\\nMindsets Change the Meaning of Effort\\nQuestions and Answers\\n3.    THE TRUTH ABOUT ABILITY AND ACCOMPLISHMENT\\nMindset and School Achievement\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[5].page_content[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497a29bb",
   "metadata": {},
   "source": [
    "## WHat is chunking\n",
    "- Chunking is about \n",
    "- - designing the right chunks that retain meaning and context, \n",
    "- - Handle multiple modality(text,table,image)\n",
    "- - Fit within model constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472564b4",
   "metadata": {},
   "source": [
    "## Types of Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114d208",
   "metadata": {},
   "source": [
    "### 1.Lenght based chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f205fd",
   "metadata": {},
   "source": [
    "#### 1.1 Character text splitter\n",
    "- First split the documents based on separator then try to get the chunk size cloer to chunk_size\n",
    "- First chunking happens based on separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63c1bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(documents[9].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f9ba97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=documents[9].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecc5e87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d626e8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2339"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5d3119e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=CharacterTextSplitter(chunk_size=10,chunk_overlap=0,separator='',length_function=lambda x:x.count(\" \"))\n",
    "chunks=splitter.split_text(sample_text)\n",
    "# Created a chunk of size 70, which is longer than the specified 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ec436a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94673a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67000484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of each chapter and throughout the last chapter,\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b74630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_doc=splitter.split_documents([documents[9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e23e527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bac39685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'calibre (4.13.0) [https://calibre-ebook.com]', 'creator': 'calibre (4.13.0) [https://calibre-ebook.com]', 'creationdate': '2020-03-29T20:43:51+00:00', 'author': 'Carol Dweck', 'moddate': '2020-03-29T23:43:59+03:00', 'title': 'Mindset - Updated Edition: Changing The Way You think To Fulfil Your Potential', 'source': 'Data/mindset.pdf', 'total_pages': 306, 'page': 9, 'page_label': '10'}, page_content='At the end')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f42dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_character_text_split(\n",
    "    text: str = None,\n",
    "    documents: list = None,\n",
    "    **kwargs\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Split text or documents into chunks using LangChain's CharacterTextSplitter.\n",
    "    \n",
    "    This function provides a flexible interface for chunking either raw text strings\n",
    "    or LangChain Document objects using character-based splitting with a specified\n",
    "    separator.\n",
    "    \n",
    "    Args:\n",
    "        text (str, optional): Raw text string to be split into chunks.\n",
    "            If provided, the function returns a list of text strings.\n",
    "        documents (list, optional): List of LangChain Document objects to be split.\n",
    "            If provided, the function returns a list of Document objects with metadata.\n",
    "        **kwargs: Additional keyword arguments passed to CharacterTextSplitter.\n",
    "            Common parameters include:\n",
    "                - chunk_size (int): Maximum size of chunks in characters. Default: 4000\n",
    "                - chunk_overlap (int): Number of overlapping characters between chunks. Default: 200\n",
    "                - separator (str): Character(s) to split on. Default: '\\n\\n'\n",
    "                - length_function (callable): Function to measure text length. Default: len\n",
    "                - is_separator_regex (bool): Whether separator is a regex pattern. Default: False\n",
    "                - keep_separator (bool): Whether to keep the separator in chunks. Default: False\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of text chunks (if text provided) or Document objects (if documents provided).\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If neither text nor documents are provided, or if both are provided.\n",
    "        TypeError: If text is not a string or documents is not a list.\n",
    "    \n",
    "    Examples:\n",
    "        >>> # Split raw text\n",
    "        >>> text = \"Line 1\\\\nLine 2\\\\nLine 3\"\n",
    "        >>> chunks = chunking_character_text_split(\n",
    "        ...     text=text,\n",
    "        ...     chunk_size=10,\n",
    "        ...     chunk_overlap=2,\n",
    "        ...     separator='\\\\n'\n",
    "        ... )\n",
    "        \n",
    "        >>> # Split documents\n",
    "        >>> from langchain.schema import Document\n",
    "        >>> docs = [Document(page_content=\"Some text\", metadata={\"source\": \"file.txt\"})]\n",
    "        >>> chunks = chunking_character_text_split(\n",
    "        ...     documents=docs,\n",
    "        ...     chunk_size=100,\n",
    "        ...     chunk_overlap=20\n",
    "        ... )\n",
    "    \n",
    "    Notes:\n",
    "        - CharacterTextSplitter splits by separator first, then tries to combine splits\n",
    "          to approach chunk_size. It will NOT split within separator-delimited sections.\n",
    "        - If a section between separators exceeds chunk_size, it will be kept as-is\n",
    "          (resulting in a chunk larger than specified).\n",
    "        - Consider using RecursiveCharacterTextSplitter for more reliable chunking\n",
    "          that respects chunk_size limits.\n",
    "    \"\"\"\n",
    "    from langchain_text_splitters import CharacterTextSplitter\n",
    "    \n",
    "    # Validation: Ensure exactly one input is provided\n",
    "    if text is None and documents is None:\n",
    "        raise ValueError(\"Either 'text' or 'documents' must be provided, but both are None.\")\n",
    "    \n",
    "    if text is not None and documents is not None:\n",
    "        raise ValueError(\"Provide either 'text' or 'documents', not both.\")\n",
    "    \n",
    "    # Type validation\n",
    "    if text is not None and not isinstance(text, str):\n",
    "        raise TypeError(f\"'text' must be a string, got {type(text).__name__}\")\n",
    "    \n",
    "    if documents is not None and not isinstance(documents, list):\n",
    "        raise TypeError(f\"'documents' must be a list, got {type(documents).__name__}\")\n",
    "    \n",
    "    # Initialize splitter\n",
    "    splitter = CharacterTextSplitter(**kwargs)\n",
    "    \n",
    "    # Process based on input type\n",
    "    if text:\n",
    "        chunks = splitter.split_text(text)\n",
    "    else:  # documents is not None\n",
    "        chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "75d2b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=chunking_character_text_split(text=sample_text,chunk_size=100,separator='',chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ac762df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'At the end of each chapter and throughout the last chapter, I show you\\nways to apply the lessons—way'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393b6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e57af4db",
   "metadata": {},
   "source": [
    "#### 1.2 Token Text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a79aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Token-Based Splitting ===\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import tiktoken python package. This is needed in order to for TokenTextSplitter. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 215\u001b[39m\n\u001b[32m    207\u001b[39m sample_text = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[33mArtificial intelligence (AI) is transforming the world. Machine learning models\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[33mcan now process vast amounts of data and make predictions with remarkable accuracy.\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[33mNatural language processing has enabled computers to understand and generate human\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[33mlanguage, opening up new possibilities for human-computer interaction.\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Token-Based Splitting ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m token_chunks = \u001b[43mchunking_token_text_split_alternative\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 50 tokens\u001b[39;49;00m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 10 token overlap\u001b[39;49;00m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcl100k_base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    220\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(token_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(token_chunks, \u001b[32m1\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mchunking_token_text_split_alternative\u001b[39m\u001b[34m(text, documents, encoding_name, **kwargs)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(documents).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Initialize TokenTextSplitter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m splitter = \u001b[43mTokenTextSplitter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Process\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\RAG\\RAG_ENV\\Lib\\site-packages\\langchain_text_splitters\\base.py:260\u001b[39m, in \u001b[36mTokenTextSplitter.__init__\u001b[39m\u001b[34m(self, encoding_name, model_name, allowed_special, disallowed_special, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _HAS_TIKTOKEN:\n\u001b[32m    255\u001b[39m     msg = (\n\u001b[32m    256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import tiktoken python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis is needed in order to for TokenTextSplitter. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    259\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    263\u001b[39m     enc = tiktoken.encoding_for_model(model_name)\n",
      "\u001b[31mImportError\u001b[39m: Could not import tiktoken python package. This is needed in order to for TokenTextSplitter. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Alternative implementation using TokenTextSplitter directly\n",
    "def chunking_token_text_split_alternative(\n",
    "    text: str = None,\n",
    "    documents: list = None,\n",
    "    encoding_name: str = \"cl100k_base\",\n",
    "    **kwargs\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Alternative implementation using TokenTextSplitter directly.\n",
    "    \n",
    "    This version uses the dedicated TokenTextSplitter class instead of\n",
    "    CharacterTextSplitter.from_tiktoken_encoder(). Both approaches work,\n",
    "    but this gives you more direct control over the tokenization process.\n",
    "    \n",
    "    Parameters and usage are identical to chunking_token_text_split().\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from langchain_text_splitters import TokenTextSplitter\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"langchain_text_splitters not found. \"\n",
    "            \"Install with: pip install langchain-text-splitters\"\n",
    "        )\n",
    "    \n",
    "    # Validation\n",
    "    if text is None and documents is None:\n",
    "        raise ValueError(\"Either 'text' or 'documents' must be provided.\")\n",
    "    \n",
    "    if text is not None and documents is not None:\n",
    "        raise ValueError(\"Provide either 'text' or 'documents', not both.\")\n",
    "    \n",
    "    if text is not None and not isinstance(text, str):\n",
    "        raise TypeError(f\"'text' must be a string, got {type(text).__name__}\")\n",
    "    \n",
    "    if documents is not None and not isinstance(documents, list):\n",
    "        raise TypeError(f\"'documents' must be a list, got {type(documents).__name__}\")\n",
    "    \n",
    "    # Initialize TokenTextSplitter\n",
    "    splitter = TokenTextSplitter(\n",
    "        encoding_name=encoding_name,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # Process\n",
    "    if text:\n",
    "        chunks = splitter.split_text(text)\n",
    "    else:\n",
    "        chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Artificial intelligence (AI) is transforming the world. Machine learning models\n",
    "can now process vast amounts of data and make predictions with remarkable accuracy.\n",
    "Natural language processing has enabled computers to understand and generate human\n",
    "language, opening up new possibilities for human-computer interaction.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Token-Based Splitting ===\")\n",
    "token_chunks = chunking_token_text_split_alternative(\n",
    "    text=sample_text,\n",
    "    chunk_size=50,  # 50 tokens\n",
    "    chunk_overlap=10,  # 10 token overlap\n",
    "    encoding_name=\"cl100k_base\"\n",
    ")\n",
    "\n",
    "print(f\"Number of chunks: {len(token_chunks)}\")\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(f\"Length: {len(chunk)} characters\")\n",
    "\n",
    "# Compare with character-based splitting\n",
    "print(\"\\n\\n=== Character-Based Splitting (for comparison) ===\")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    chunk_size=200,  # 200 characters\n",
    "    chunk_overlap=40,\n",
    "    separator=\" \"\n",
    ")\n",
    "char_chunks = char_splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Number of chunks: {len(char_chunks)}\")\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(f\"Length: {len(chunk)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332dd34",
   "metadata": {},
   "source": [
    "## 2. Text Structure based chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673b5d9",
   "metadata": {},
   "source": [
    "#### 2.1 Recursive Character Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "339a15ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da427b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "LangGraph is a powerful library for building stateful, multi-actor applications with LLMs. \n",
    "\n",
    "It excels at creating complex agentic workflows that require multiple steps and decision points. Unlike simple chains, LangGraph maintains conversation history and state across nodes.\n",
    "\n",
    "Key features include conditional edges, tool calling, human-in-the-loop approval, and streaming support. Developers use it for NLQ-to-SQL agents, customer support bots, and data analysis pipelines.\n",
    "\n",
    "The library integrates seamlessly with LangChain tools, OpenAI models, and vector stores. Production deployments use FastAPI endpoints that invoke compiled graphs with custom AgentState schemas.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "797a42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=40)\n",
    "chunks=splitter.split_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1a7bbddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "LangGraph is a powerful library for building stateful, multi-actor applications with LLMs.\n",
      "****************************************************************************************************\n",
      "It excels at creating complex agentic workflows that require multiple steps and decision points. Unlike simple chains, LangGraph maintains conversation history and state across nodes.\n",
      "****************************************************************************************************\n",
      "Key features include conditional edges, tool calling, human-in-the-loop approval, and streaming support. Developers use it for NLQ-to-SQL agents, customer support bots, and data analysis pipelines.\n",
      "****************************************************************************************************\n",
      "The library integrates seamlessly with LangChain tools, OpenAI models, and vector stores. Production deployments use FastAPI endpoints that invoke compiled graphs with custom AgentState schemas.\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print('*'*100)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9e8e4",
   "metadata": {},
   "source": [
    "#### 2.2 MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1664e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a88a221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text='''\n",
    "# Product Manual\n",
    "\n",
    "## Installation\n",
    "\n",
    "1. Download the installer\n",
    "2. Run `python setup.py install`\n",
    "3. Verify: `pip show mypackage`\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Basic Query\n",
    "```python\n",
    "result = query(\"SELECT * FROM users\")\n",
    "print(result)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "53a60d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\",\"H1\"),(\"##\",\"H2\"),(\"###\",\"H3\")])\n",
    "chunks=splitter.split_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e01eafa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "1. Download the installer\n",
      "2. Run `python setup.py install`\n",
      "3. Verify: `pip show mypackage`\n",
      "{'H1': 'Product Manual', 'H2': 'Installation'}\n",
      "****************************************************************************************************\n",
      "```python\n",
      "result = query(\"SELECT * FROM users\")\n",
      "print(result)\n",
      "\n",
      "\n",
      "{'H1': 'Product Manual', 'H2': 'Usage Examples', 'H3': 'Basic Query'}\n"
     ]
    }
   ],
   "source": [
    "for i in chunks:\n",
    "    print('*'*100)\n",
    "    print(i.page_content)\n",
    "    print(i.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dafcd5",
   "metadata": {},
   "source": [
    "#### 2.3 HTML Header Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83721e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c79db9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=''' \n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <h1>Main Document</h1>\n",
    "    <p>Introduction paragraph</p>\n",
    "    \n",
    "    <h2>Chapter 1</h2>\n",
    "    <p>Chapter 1 content</p>\n",
    "    \n",
    "    <h3>Section 1.1</h3>\n",
    "    <p>Section content</p>\n",
    "    \n",
    "    <h2>Chapter 2</h2>\n",
    "    <p>Chapter 2 content</p>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f0507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on\n",
    ")\n",
    "\n",
    "chunks = html_splitter.split_text(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99760f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "page_content='Main Document' metadata={'Header 1': 'Main Document'}\n",
      "****************************************************************************************************\n",
      "page_content='Introduction paragraph' metadata={'Header 1': 'Main Document'}\n",
      "****************************************************************************************************\n",
      "page_content='Chapter 1' metadata={'Header 1': 'Main Document', 'Header 2': 'Chapter 1'}\n",
      "****************************************************************************************************\n",
      "page_content='Chapter 1 content' metadata={'Header 1': 'Main Document', 'Header 2': 'Chapter 1'}\n",
      "****************************************************************************************************\n",
      "page_content='Section 1.1' metadata={'Header 1': 'Main Document', 'Header 2': 'Chapter 1', 'Header 3': 'Section 1.1'}\n",
      "****************************************************************************************************\n",
      "page_content='Section content' metadata={'Header 1': 'Main Document', 'Header 2': 'Chapter 1', 'Header 3': 'Section 1.1'}\n",
      "****************************************************************************************************\n",
      "page_content='Chapter 2' metadata={'Header 1': 'Main Document', 'Header 2': 'Chapter 2'}\n",
      "****************************************************************************************************\n",
      "page_content='Chapter 2 content' metadata={'Header 1': 'Main Document', 'Header 2': 'Chapter 2'}\n"
     ]
    }
   ],
   "source": [
    "for i in chunks:\n",
    "    print('*'*100)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78a222",
   "metadata": {},
   "source": [
    "### COde specifit splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7660fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee8281f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code = \"\"\"\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "    \n",
    "    def method1(self):\n",
    "        return self.value\n",
    "    \n",
    "    def method2(self):\n",
    "        self.value += 1\n",
    "\n",
    "def standalone_function():\n",
    "    return \"Hello\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77e374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c4f9a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = python_splitter.split_text(python_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b05f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "class MyClass:\n",
      "    def __init__(self):\n",
      "        self.value = 0\n",
      "****************************************************************************************************\n",
      "def method1(self):\n",
      "        return self.value\n",
      "\n",
      "    def method2(self):\n",
      "        self.value += 1\n",
      "****************************************************************************************************\n",
      "def standalone_function():\n",
      "    return \"Hello\"\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print('*'*100)\n",
    "    print(chunk)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce8255c",
   "metadata": {},
   "source": [
    "### Semantic Based Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3c1030",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SemanticChunker' from 'langchain_text_splitters' (d:\\Learning\\RAG\\RAG_ENV\\Lib\\site-packages\\langchain_text_splitters\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SemanticChunker\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'SemanticChunker' from 'langchain_text_splitters' (d:\\Learning\\RAG\\RAG_ENV\\Lib\\site-packages\\langchain_text_splitters\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "093164f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SemanticChunker' from 'langchain_text_splitters' (d:\\Learning\\RAG\\RAG_ENV\\Lib\\site-packages\\langchain_text_splitters\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SemanticChunker\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[32m      4\u001b[39m text = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mThe dog ran in the park. It was very happy. The weather was sunny.\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mMachine learning is a subset of AI. Neural networks are powerful.\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mThe cat slept on the couch. It purred softly.\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'SemanticChunker' from 'langchain_text_splitters' (d:\\Learning\\RAG\\RAG_ENV\\Lib\\site-packages\\langchain_text_splitters\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "text = \"\"\"\n",
    "The dog ran in the park. It was very happy. The weather was sunny.\n",
    "Machine learning is a subset of AI. Neural networks are powerful.\n",
    "The cat slept on the couch. It purred softly.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73872b",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95\n",
    ")\n",
    "\n",
    "chunks = semantic_splitter.split_text(text)\n",
    "# Groups semantically similar sentences together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c77ad",
   "metadata": {},
   "source": [
    "### Chunk evaluation\n",
    "\n",
    "Use[ chuunkviz ](https://chunkviz.up.railway.app/]) to visualize the quality of chunking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
